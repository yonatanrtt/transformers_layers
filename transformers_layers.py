# -*- coding: utf-8 -*-
"""transformers_layers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18P_V6h8pxsPGnF5veG3HzaPaO8lYGllt
"""

!pip install datasets transformers wandb --quiet

import sys
import os

DRIVE_PATH = "/content/drive/MyDrive/transformers_layers/"
sys.path.append(DRIVE_PATH)

import torch

if torch.cuda.is_available():
  print("cuda")
  device = torch.device('cuda')

import data
import models
import lm_utils
from lm_utils import LM

copa, cb, rte, wic = data.get_datasets()

lm: LM = lm_utils.init()
lm.model.to(device)
train_dataloader = data.get_dataloader(copa["train"], lm, device)

from tqdm.auto import tqdm
tqdm.pandas()

superglue_model = models.CopaModel(lm, device)
for step, batch in tqdm(enumerate(train_dataloader)):
  positive_tokenized, negative_tokenized, positive_input, negative_input = batch
  superglue_model(positive_tokenized, negative_tokenized, positive_input, negative_input)