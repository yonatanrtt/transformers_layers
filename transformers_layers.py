# -*- coding: utf-8 -*-
"""transformers_layers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18P_V6h8pxsPGnF5veG3HzaPaO8lYGllt
"""

!pip install datasets transformers wandb --quiet

import sys
import os

DRIVE_PATH = "/content/drive/MyDrive/transformers_layers/"
sys.path.append(DRIVE_PATH)

import torch

if torch.cuda.is_available():
  print("cuda")
  device = torch.device('cuda')

import data
import models
import lm

copa, cb, rte, wic = data.get_datasets()

tokenizer, config, model, model_mlm = lm.init()
 model.to(device)
 train_dataloader = data.get_dataloader(copa["train"], tokenizer)

superglue_model = models.CopaModel(model, config, device)
for step, batch in enumerate(train_dataloader):
  positive, negative = torch.stack(batch).to(device)
  print("-"*100)
  superglue_model(positive, negative)